---
title: "Next Word Predictor"
author: "Anvil"
date: "24/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

# Summary

This project will use machine learning and natural language processing to build a "next word" predictive model such as the ones we have on our smartphones keyboards. For that, we use a database provided by SwiftKey. Our whole research will be made on R.

# Loading packages

This project will use the following packages ; if you want to reproduce this research, make sure to download them :

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(e1071)
library(quanteda)
library(markovchain)
library(data.table)
library(knitr) # Md document aesthetics
library(kableExtra) # Md document aesthetics
```

# Loading the data

As a personal habit, I automate the download, unzipping and loading process to make the project reproducible. This chapter will create a "data" folder in your current working directory and download SwiftKey's training dataset there.

```{r}
if (!file.exists("data")) {
   dir.create("data")
}
urlfile <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
filename <- "./data/swiftkey_db.zip"

if (!file.exists(filename)) {
   download.file(urlfile, filename, method = "curl")
}

dataset.name <- "final"
if (!file.exists(dataset.name)) {
   unzip(filename)
}
```

The unzipped folder contains blog, news and twitter text for 4 different languages : english, german, finnish and russian. That's a total of 12 datasets, all collected by a web crawler, and we will only focus on the english ones.

```{r, cache=T}

blogcon <- file("./final/en_US/en_US.blogs.txt", "r") 
blogtext <- readLines(blogcon, encoding = "UTF-8") 

newscon <- file("./final/en_US/en_US.news.txt", "r") 
newstext <- readLines(newscon, encoding = "UTF-8") 

twitcon <- file("./final/en_US/en_US.twitter.txt", "r") 
twittext <- readLines(twitcon, encoding = "UTF-8") 

close(blogcon)
close(newscon)
close(twitcon)

```

# A bit of exploratory data analysis and pre-processing

When dealing with our case of text analytics, pre-processing and exploration will be tightly linked. We will often need to do some processing in order to explore the data. 

Moreover, we want as little bias as possible in our model, so we will split our datasets into a training and testing set as soon as possible and do the exploration only on the training set.

## Structure

Now that the text files are open, we might want to take a look at the content, starting at the datasets' sizes :

```{r, cache=T}
summarydf <- data.frame(
   Object = c("Blog dataset", "News dataset", "Twitter dataset"),
   Nr.of.elements = sapply(list(blogtext, newstext, twittext), length),
   length.longest.element = c(
      max(sapply(blogtext, nchar)),
      max(sapply(newstext, nchar)),
      max(sapply(twittext, nchar))
   ),
   dataset.size = sapply(list(blogtext, newstext, twittext), object.size)
)

kable(summarydf) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

We have extremely large datasets, taking a total of more than 600MB of memory. While the twitter datasets only has small entries with a maximum of 140 characters, it has more that 2 millions of them. The blog dataset, on the contrary, has large elements, the biggest one having more than 40 thousand characters, while still having close to a million entries.

Considering the sizes of the datasets, we might not want to use 100% of the available data in our model building. This is good, it means that we can be selective in our filtering and pre-processing and still end up with a large enough database to build an efficient model.

We'll start by building a training set that consists of 10% of each dataset.

```{r, cache=T}
set.seed(1234)
blogtrain <- sample(blogtext, 0.1*length(blogtext))


newstrain <- sample(blogtext, 0.1*length(newstext)) 


twittrain <- sample(blogtext, 0.1*length(twittext)) 


training <- c(blogtrain, newstrain, twittrain)

# removing obsolete datasets to save some RAM
rm(blogtext)
rm(twittext)
rm(newstext)
rm(blogtrain)
rm(newstrain)
rm(twittrain)
```

## What's next ?

Now that we have datasets to work on, we'll manipulate them a little. The following steps have one aim : to create a frequency matrix, which displays the frequency with which every word is used. With that, we'll know on what our model can be built on. 

For instance, if we find out that we can cover 90% of texts with only a small subset of words, we can greatly reduce the complexity of our model by having a high accuracy on these words and an average one on the others.

## Tokenization

The first thing we will do with our training set is tokenization : we will split our lines of text into chunks of words. For instance, we want the sentence "This watch is Mike's" to become ["this" "watch" "is" "mike" "s"].

This is where the package quanteda comes in ; it can automate this process with its tokens() function. We will set the following parameters :

- Do not tokenize numbers
- Do not tokenize punctuation
- Do not tokenize symbols such as dollar signs or hashtags
- Do not tokenize URLS
- Do not tokenize twitter words such as "rt"
- Split hyphenated words

```{r, cache=T}
train_tokens <- tokens(training, what = "word", remove_numbers = T,
                       remove_punct = T, remove_symbols = T, split_hyphens = T,
                       remove_url = T, remove_twitter = T)
```


Let's check how that changed one of our lines :

```{r}
training[1]
train_tokens[[1]]
```

Finally, to get the full transformation we wanted, we convert all of the tokens to lower case :

```{r, cache=T}
train_tokens <- tokens_tolower(train_tokens)
```


## Token processing

### Stopwords

In every language, some words are here for grammatical purposes and are not the ones that carry the message of the sentence. These words are called stopwords, and their presence can confuse our predictive model, so we'll filter them out.

An example of a stopword in the english language is "the". Here's a sample of the built-in list of stopwords provided by the quanteda package

```{r}
stopwords(language = "en")[1:10]
```

We'll use that to filter them out in our dataset :

```{r, cache=T}
train_tokens <- tokens_select(train_tokens, stopwords(), selection = "remove")
```

Let's take the first element and see how it has changed now :

```{r}
train_tokens[[1]]
```

### Profanity filtering

We'll get a bit politically correct here, and prevent our model from both taking profanities into account and suggesting them as predictions. Since we use a tweeter database we're bound to encounter some otherwise.

There's a public list of profanities available [here](https://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/) for webmasters or moderators to use, and this is what we'll use here to filter them out.

```{r, cache=T}
# Getting the list
profanities <- read.csv(
   "https://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv", 
                        header = F, sep = ",", skip = 4)
profanities[, 2] <- gsub(pattern = ",", replacement = "", profanities[, 2])
profanities <- profanities[, 2]

# Filtering

train_tokens <- tokens_select(train_tokens, profanities, selection = "remove")
```




## Frequency matrix

We can now create our frequency matrix - or document feature matrix (DFM). Since our dataset has had quite some steps of pre-processing, we do not need a lot of parameters when creating it.

```{r, cache=T}
unigram <- dfm(train_tokens, tolower = F)
```

We can now see the most frequent words in our dataset :

```{r}
unifreq <- textstat_frequency(unigram)
head(unifreq)
top50 <- filter(unifreq, rank <= 50)
qplot(reorder(feature, rank), frequency, data = top50) + 
   labs(x = "Word", y = "Frequency") + ggtitle("Most frequent words (top 50)") +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
         plot.title = element_text(hjust = 0.5))
```



Now that we saw the top 50 words, let's see a more detailed breakdown of the frequencies in our dataset :

```{r}
quantile(unifreq$frequency)
```

75% of the words in our dataset appear 6 times or less. 

We'll create a dataset that shows the count of each words in our training corpus, and we set a benchmark of at least 4 total counts. The words below that will be removed, and this will greatly reduce our training set's size :

```{r, cache=T}
unifreq <- data.table(word = unifreq$feature, count = unifreq$frequency)
unifreq <- unifreq[count > 4, ]
```



# Markov Chains and N-Grams

What we plan on doing here is to build a predictive model using Markov Chains, which will interpret text as a "current state", with a probability of having a next word as the "next state". 

For example, when looking at the sentence "the dog chases after the fat cat", each word has a probability of leading to the next :

![](./data/img/1markov.PNG)

The word "the" has a 50% chance of leading to "dog" and 50% chance of leading to "fat", while the other words only have one result with a probability of 100%. 

This is how our model will train on the text we have. However, we can already see the limitations of this method : each word will have many results and if we do not take the previous one into account, we'll have a high chance of predicting gibberish. This is why the "current state" has to be a result of several words. If we do that on the same example :

![](./data/img/2markov.PNG)

Here what is taken into account are pairs of words, which are called digrams. While "The" still has 2 possible results, the state "After The" only has one resulting outcome, "Fat". 

Therefore, what we will now do is take our tokens list and make it into 2-grams and 3-grams token lists, only keep as subset of combinations that appear at least 3 times, and build three models combined into one in the following way :

- If the user enters 3 words, the model built on 3-grams will be called
- If the user enters 2 words or if the 3-grams prediction fails, the model built on 2-grams will be called
- If the user only enters 1 word or if the other models fail, the model built on 1-grams will be called.

# Building n-grams

### Building the 2-grams token list 

```{r, cache=T}
train_tokens_2G <- tokens_ngrams(train_tokens, n = 2)
bigram <- dfm(train_tokens_2G, tolower = F)
bifreq <- textstat_frequency(bigram)
bifreq <- data.table(word = bifreq$feature, count = bifreq$frequency)
bifreq <- bifreq[count > 4, ]
```

### Building the 3-grams token list 

```{r, cache=T}
train_tokens_3G <- tokens_ngrams(train_tokens, n = 3)
trigram <- dfm(train_tokens_3G, tolower = F)
trifreq <- textstat_frequency(trigram)
trifreq <- data.table(word = trifreq$feature, count = trifreq$frequency)
trifreq <- trifreq[count > 4, ]
```

We now have everything we need to build our model

